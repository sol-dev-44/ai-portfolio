# backend/requirements.txt
# Full dependencies for main.py + agent_langchain.py + lsat_service.py

# FastAPI + Server
fastapi>=0.109.0
uvicorn[standard]>=0.27.0
fastapi==0.115.6
uvicorn==0.34.0
pydantic==2.10.4
python-dotenv==1.0.1
supabase==2.11.0
openai==1.58.1

# Tokenizers (for existing tokenizer endpoints)
tiktoken==0.8.0

# ML Models (for generation visualization)
torch==2.5.1
transformers==4.47.1

# LangChain ecosystem (for local agent)
langchain==0.3.13
langchain-community==0.3.13
langchain-anthropic==0.3.3

# Data Fetching
datasets==3.2.0

# HTTP Client (for LSAT service Claude API calls)
httpx==0.28.1

# =============================================================================
# SETUP NOTES
# =============================================================================
# 
# 1. For LOCAL development with GPU:
#    pip install -r requirements.txt
#
# 2. For Railway/production (CPU only, smaller footprint):
#    Consider using torch-cpu or excluding heavy models
#
# 3. For Ollama (local LLM):
#    brew install ollama  (macOS)
#    ollama serve &
#    ollama pull llama3
#
# 4. The agent has TWO backends:
#    - Claude: Uses ANTHROPIC_API_KEY via Next.js (no Python deps)
#    - LangChain: Uses Ollama locally (requires langchain + ollama running)
#
# 5. LSAT Service:
#    - Run standalone: uvicorn lsat_service:app --port 8001
#    - Or integrate endpoints into main.py
#    - Uses httpx for async Claude API streaming
# =============================================================================
# Architecture Overview

## System Design

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          FRONTEND                                â”‚
â”‚                     (Next.js + React)                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                   â”‚
â”‚  /generation/page.tsx                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚  â”‚  1. User types: "The future of AI is"      â”‚                 â”‚
â”‚  â”‚     â†“ (500ms debounce)                     â”‚                 â”‚
â”‚  â”‚  2. useLazyGetProbabilitiesQuery()         â”‚                 â”‚
â”‚  â”‚     â†“                                      â”‚                 â”‚
â”‚  â”‚  3. RTK Query dispatches API call          â”‚                 â”‚
â”‚  â”‚     â†“                                      â”‚                 â”‚
â”‚  â”‚  4. Gets response with probabilities       â”‚                 â”‚
â”‚  â”‚     â†“                                      â”‚                 â”‚
â”‚  â”‚  5. D3.js renders bar chart                â”‚                 â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚                          â†•                                       â”‚
â”‚                                                                   â”‚
â”‚  store/api/generation.ts                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚  â”‚  - TypeScript types                        â”‚                 â”‚
â”‚  â”‚  - RTK Query endpoints                     â”‚                 â”‚
â”‚  â”‚  - Caching (5 min TTL)                     â”‚                 â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚                          â†•                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†•
                   HTTP POST Request
                           â†•
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          BACKEND                                 â”‚
â”‚                     (FastAPI + PyTorch)                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                   â”‚
â”‚  POST /api/generation/probabilities                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚  â”‚  Request:                                  â”‚                 â”‚
â”‚  â”‚  {                                         â”‚                 â”‚
â”‚  â”‚    "prompt": "The future of AI is",       â”‚                 â”‚
â”‚  â”‚    "top_k": 10                             â”‚                 â”‚
â”‚  â”‚  }                                         â”‚                 â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚                          â†“                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚  â”‚  1. Tokenize prompt                        â”‚                 â”‚
â”‚  â”‚     GPT-2 tokenizer                        â”‚                 â”‚
â”‚  â”‚     "The future of AI is" â†’ [464, 2003,â€¦]  â”‚                 â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚                          â†“                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚  â”‚  2. Forward pass through GPT-2             â”‚                 â”‚
â”‚  â”‚     model(**inputs) â†’ logits               â”‚                 â”‚
â”‚  â”‚     Shape: [1, seq_len, 50257]             â”‚                 â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚                          â†“                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚  â”‚  3. Extract next token logits              â”‚                 â”‚
â”‚  â”‚     logits[0, -1, :] (last position)       â”‚                 â”‚
â”‚  â”‚     Shape: [50257] (all vocab)             â”‚                 â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚                          â†“                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚  â”‚  4. Softmax to get probabilities           â”‚                 â”‚
â”‚  â”‚     probabilities = softmax(logits)        â”‚                 â”‚
â”‚  â”‚     [0.15, 0.12, 0.10, ...]                â”‚                 â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚                          â†“                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚  â”‚  5. Get top-k tokens                       â”‚                 â”‚
â”‚  â”‚     torch.topk(probabilities, k=10)        â”‚                 â”‚
â”‚  â”‚     Returns indices & probabilities        â”‚                 â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚                          â†“                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚  â”‚  6. Decode tokens & format response        â”‚                 â”‚
â”‚  â”‚     [6016] â†’ " bright" (15.2%)             â”‚                 â”‚
â”‚  â”‚     [1016] â†’ " going" (12.4%)              â”‚                 â”‚
â”‚  â”‚     ...                                    â”‚                 â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚                          â†“                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚  â”‚  Response:                                 â”‚                 â”‚
â”‚  â”‚  {                                         â”‚                 â”‚
â”‚  â”‚    "prompt": "The future of AI is",       â”‚                 â”‚
â”‚  â”‚    "top_tokens": [                         â”‚                 â”‚
â”‚  â”‚      {"token": " bright", ...},            â”‚                 â”‚
â”‚  â”‚      {"token": " going", ...}              â”‚                 â”‚
â”‚  â”‚    ],                                      â”‚                 â”‚
â”‚  â”‚    "total_tokens_considered": 50257        â”‚                 â”‚
â”‚  â”‚  }                                         â”‚                 â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚                                                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Data Flow

### Request Flow
```
User Input
   â†“
Debounce (500ms)
   â†“
RTK Query (POST /api/generation/probabilities)
   â†“
FastAPI Endpoint
   â†“
GPT-2 Model (forward pass)
   â†“
Softmax (probabilities)
   â†“
Top-k Selection
   â†“
JSON Response
   â†“
RTK Query Cache
   â†“
React State Update
   â†“
D3.js Re-render
```

### Data Transformation
```
"The future of AI is"
   â†“ (tokenize)
[464, 2003, 286, 9552, 318]
   â†“ (model forward)
logits: Tensor[50257]
   â†“ (softmax)
probabilities: [0.15, 0.12, 0.10, ...]
   â†“ (top-k)
top_indices: [6016, 1016, 407, ...]
top_probs: [0.15, 0.12, 0.10, ...]
   â†“ (decode + format)
[
  {token: " bright", probability: 0.15},
  {token: " going", probability: 0.12},
  ...
]
```

---

## Component Structure

### Backend (`main.py`)
```
FastAPI App
â”œâ”€â”€ CORS Middleware
â”œâ”€â”€ Tokenizer Initialization (tiktoken)
â”œâ”€â”€ GPT-2 Initialization (PyTorch)
â”œâ”€â”€ Routes
â”‚   â”œâ”€â”€ GET  /
â”‚   â”œâ”€â”€ GET  /health
â”‚   â”œâ”€â”€ GET  /api/tokenizers
â”‚   â”œâ”€â”€ POST /api/tokenize
â”‚   â””â”€â”€ POST /api/generation/probabilities â† NEW
â””â”€â”€ Models (Pydantic)
    â”œâ”€â”€ TokenizeRequest
    â”œâ”€â”€ TokenizerResult
    â”œâ”€â”€ ProbabilityRequest â† NEW
    â”œâ”€â”€ TokenProbability â† NEW
    â””â”€â”€ ProbabilityResponse â† NEW
```

### Frontend Store
```
Redux Store
â”œâ”€â”€ tokenizerApi
â”‚   â”œâ”€â”€ getTokenizers
â”‚   â””â”€â”€ tokenize
â””â”€â”€ generationApi â† NEW
    â””â”€â”€ getProbabilities
```

### Frontend Page (`/generation`)
```
GenerationPage
â”œâ”€â”€ State
â”‚   â”œâ”€â”€ prompt (input text)
â”‚   â””â”€â”€ debouncedPrompt
â”œâ”€â”€ API Hooks
â”‚   â””â”€â”€ useLazyGetProbabilitiesQuery
â”œâ”€â”€ Effects
â”‚   â”œâ”€â”€ Debounce input â†’ debouncedPrompt
â”‚   â”œâ”€â”€ Trigger API on debounce
â”‚   â””â”€â”€ D3.js visualization on data change
â””â”€â”€ UI
    â”œâ”€â”€ Header
    â”œâ”€â”€ Input (with loading state)
    â”œâ”€â”€ SVG Chart (D3.js)
    â””â”€â”€ Stats Cards
```

---

## Technology Stack

### Backend
- **FastAPI**: Web framework
- **PyTorch**: Deep learning framework
- **Transformers**: Hugging Face library
- **tiktoken**: Tokenization (for tokenizer feature)
- **Pydantic**: Data validation

### Frontend
- **Next.js 14**: React framework
- **TypeScript**: Type safety
- **RTK Query**: API state management
- **D3.js**: Data visualization
- **Framer Motion**: Animations
- **Tailwind CSS**: Styling

---

## Performance Characteristics

### Backend
```
Model Loading:     5-10s (one-time on startup)
Memory Usage:      ~500MB (GPT-2)
First Request:     ~500ms (model warming)
Cached Request:    ~50ms
Regular Request:   200-300ms
Concurrent Limit:  10 requests
```

### Frontend
```
Debounce Delay:    500ms
API Call:          200-500ms (backend dependent)
D3 Render:         ~50ms
Cache TTL:         5 minutes
Total UX Latency:  700-1000ms (feels instant)
```

---

## Scaling Considerations

### Current (Railway Free Tier)
- âœ… Single instance
- âœ… CPU inference
- âœ… ~500ms response time
- âœ… Good for demo/portfolio

### Future (If Needed)
- ğŸš€ GPU instance (10-50x faster)
- ğŸš€ Model quantization (smaller memory)
- ğŸš€ Redis caching (faster repeated queries)
- ğŸš€ Load balancing (multiple instances)

---

## Error Handling

### Backend
```python
try:
    # Get probabilities
except HTTPException:
    # Return 400/503/500 with detail
except Exception:
    # Return 500 with error message
```

### Frontend
```typescript
const { data, error, isFetching } = useQuery();

if (error) return <ErrorMessage />
if (isFetching) return <LoadingSpinner />
if (data) return <Visualization />
```

---

## Security

### CORS
- Configured for localhost:3000
- Regex pattern for Vercel preview branches
- Specific origin for production

### Input Validation
- Max prompt length: 500 chars
- Top-k range: 5-50
- Pydantic validation on all inputs

### Rate Limiting
- Current: None (dev/demo)
- Future: Can add per-IP limits if needed

---

## Deployment

### Local Development
```
Backend:  localhost:8080
Frontend: localhost:3000
```

### Production (Railway + Vercel)
```
Backend:  https://ai-portfolio-production-7eb9.up.railway.app
Frontend: https://ai-portfolio-psi-lyart.vercel.app
```

---

## Next Iteration Hooks

The current architecture makes it easy to add:

1. **Multiple strategies**: Just return selection info for each
2. **Step-by-step generation**: Add state parameter, return sequence
3. **Interactive controls**: Add temperature, top_k, top_p params
4. **Beam search viz**: Return beam tree structure
5. **3D visualization**: Same data, different D3/three.js renderer

All extensions build on the same endpoint pattern!